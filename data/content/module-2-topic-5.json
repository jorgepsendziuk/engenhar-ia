{
  "id": "module-2-topic-5",
  "title": "RAG avançado na prática",
  "content": "## Introdução\n\nNo [Módulo 1, Tópico 9](/topic/module-1-topic-9), exploramos os fundamentos de RAG (Retrieval-Augmented Generation), embeddings e busca semântica. Construímos um sistema RAG básico usando JavaScript e PostgreSQL. Agora, vamos aprofundar essas técnicas e explorar ferramentas profissionais que facilitam a construção de sistemas RAG robustos e escaláveis em produção.\n\nEste tópico cobre:\n\n1. **Orquestração de Fluxos**: Usando LangChain e Vercel AI SDK para gerenciar pipelines complexos\n2. **Gerenciamento de Erros**: Estratégias robustas para lidar com falhas\n3. **Observabilidade**: Monitoramento, logging e depuração de sistemas RAG\n\n## Revisão: RAG Básico\n\nLembrando do Módulo 1, um sistema RAG básico funciona assim:\n\n```\n1. Usuário faz pergunta\n2. Sistema converte pergunta em embedding\n3. Busca documentos similares no vector database\n4. Adiciona documentos ao contexto do LLM\n5. LLM gera resposta baseada no contexto\n```\n\nSistemas RAG em produção precisam de muito mais: orquestração, tratamento de erros, observabilidade, otimização e muito mais.\n\n## Orquestração de Fluxos com LangChain\n\n### O que é LangChain?\n\n**LangChain** é um framework para construir aplicações com LLMs. Ele fornece:\n\n- **Abstrações**: Componentes reutilizáveis (chains, agents, tools)\n- **Integrações**: Conectores para múltiplos provedores e serviços\n- **Orquestração**: Gerenciamento de fluxos complexos\n- **Memória**: Gerenciamento de estado e contexto\n\n### Conceitos Fundamentais do LangChain\n\n#### 1. Components (Componentes)\n\nComponentes básicos que podem ser combinados:\n\n- **LLMs**: Modelos de linguagem\n- **Prompts**: Templates de prompts\n- **Chains**: Sequências de componentes\n- **Agents**: Sistemas que usam tools\n- **Memory**: Armazenamento de estado\n\n#### 2. Chains (Cadeias)\n\nChains conectam múltiplos componentes:\n\n```javascript\n// Exemplo conceitual (LangChain é principalmente Python, mas há versões JS)\nimport { LLMChain } from 'langchain/chains';\nimport { PromptTemplate } from 'langchain/prompts';\n\nconst prompt = PromptTemplate.fromTemplate(\n  'Responda a pergunta baseado no contexto:\\n\\nContexto: {context}\\n\\nPergunta: {question}'\n);\n\nconst chain = new LLMChain({\n  llm: llm,\n  prompt: prompt\n});\n\nconst result = await chain.call({\n  context: '...',\n  question: '...'\n});\n```\n\n### RAG com LangChain\n\nLangChain simplifica muito a construção de RAG:\n\n```javascript\n// Exemplo com LangChain.js (versão JavaScript)\nimport { ChatOpenAI } from 'langchain/chat_models/openai';\nimport { RetrievalQAChain } from 'langchain/chains';\nimport { VectorStoreRetriever } from 'langchain/vectorstores';\n\n// 1. Configurar LLM\nconst llm = new ChatOpenAI({\n  modelName: 'gpt-4',\n  temperature: 0\n});\n\n// 2. Configurar retriever (busca)\nconst retriever = new VectorStoreRetriever({\n  vectorStore: vectorStore,\n  k: 5 // Top 5 documentos\n});\n\n// 3. Criar chain RAG\nconst ragChain = RetrievalQAChain.fromLLM(llm, retriever);\n\n// 4. Usar\nconst result = await ragChain.call({\n  query: 'O que é JavaScript?'\n});\n```\n\n### Chains Personalizadas\n\nVocê pode criar chains customizadas para fluxos complexos:\n\n```javascript\nimport { SequentialChain, LLMChain } from 'langchain/chains';\n\n// Chain 1: Buscar contexto\nconst retrievalChain = new LLMChain({\n  llm: llm,\n  prompt: retrievalPrompt\n});\n\n// Chain 2: Gerar resposta\nconst generationChain = new LLMChain({\n  llm: llm,\n  prompt: generationPrompt\n});\n\n// Chain 3: Validar resposta\nconst validationChain = new LLMChain({\n  llm: llm,\n  prompt: validationPrompt\n});\n\n// Combinar em sequência\nconst customRAGChain = new SequentialChain({\n  chains: [retrievalChain, generationChain, validationChain],\n  inputVariables: ['query'],\n  outputVariables: ['answer', 'validation']\n});\n```\n\n### Agents com Tools\n\nAgents podem usar ferramentas externas:\n\n```javascript\nimport { initializeAgentExecutorWithOptions } from 'langchain/agents';\nimport { SerpAPI } from 'langchain/tools';\nimport { Calculator } from 'langchain/tools/calculator';\n\nconst tools = [\n  new SerpAPI(), // Busca na web\n  new Calculator(), // Calculadora\n  new VectorStoreQATool(vectorStore) // RAG tool\n];\n\nconst executor = await initializeAgentExecutorWithOptions(\n  tools,\n  llm,\n  { agentType: 'zero-shot-react-description' }\n);\n\nconst result = await executor.run(\n  'Busque informações sobre JavaScript e calcule quantos anos tem desde seu lançamento'\n);\n```\n\n## Vercel AI SDK / Pipes\n\n### O que é Vercel AI SDK?\n\n**Vercel AI SDK** é uma biblioteca moderna para construir aplicações de IA, focada em:\n\n- **Streaming**: Respostas em tempo real\n- **React Integration**: Componentes React para UI\n- **Multi-provider**: Suporte a múltiplos provedores\n- **Simplicidade**: API mais simples que LangChain\n\n### RAG com Vercel AI SDK\n\n```javascript\nimport { createDataStreamResponse } from 'ai';\nimport { openai } from '@ai-sdk/openai';\nimport { vectorStore } from './vector-store';\n\nexport async function POST(req) {\n  const { messages } = await req.json();\n  const lastMessage = messages[messages.length - 1];\n  \n  // 1. Buscar contexto relevante\n  const relevantDocs = await vectorStore.similaritySearch(\n    lastMessage.content,\n    5\n  );\n  \n  // 2. Construir contexto\n  const context = relevantDocs\n    .map(doc => doc.pageContent)\n    .join('\\n\\n');\n  \n  // 3. Gerar resposta com streaming\n  const result = await openai.chat.completions.create({\n    model: 'gpt-4',\n    messages: [\n      {\n        role: 'system',\n        content: `Você é um assistente útil. Use o seguinte contexto:\\n\\n${context}`\n      },\n      ...messages\n    ],\n    stream: true\n  });\n  \n  // 4. Retornar stream\n  return createDataStreamResponse(result);\n}\n```\n\n### Vercel AI SDK com React\n\n```jsx\n'use client';\n\nimport { useChat } from 'ai/react';\n\nexport default function Chat() {\n  const { messages, input, handleInputChange, handleSubmit } = useChat();\n  \n  return (\n    <div>\n      {messages.map(message => (\n        <div key={message.id}>\n          <strong>{message.role}:</strong> {message.content}\n        </div>\n      ))}\n      \n      <form onSubmit={handleSubmit}>\n        <input\n          value={input}\n          onChange={handleInputChange}\n          placeholder=\"Faça uma pergunta...\"\n        />\n        <button type=\"submit\">Enviar</button>\n      </form>\n    </div>\n  );\n}\n```\n\n### Comparação: LangChain vs Vercel AI SDK\n\n| Característica | LangChain | Vercel AI SDK |\n|----------------|-----------|---------------|\n| Complexidade | Alta (muitas abstrações) | Baixa (API simples) |\n| Flexibilidade | Muito alta | Moderada |\n| Streaming | Suportado | Nativo e otimizado |\n| React | Não nativo | Integração nativa |\n| Ecossistema | Muito grande | Menor, mas crescente |\n| Casos de uso | Sistemas complexos | Apps web modernos |\n\n**Quando usar cada um**:\n\n- **LangChain**: Sistemas complexos, múltiplas integrações, agents avançados\n- **Vercel AI SDK**: Apps web modernos, streaming, integração React\n\n## Gerenciamento de Erros e Re-tentativas\n\n### Estratégias de Tratamento de Erros\n\nSistemas RAG podem falhar em múltiplos pontos:\n\n1. **Falha na busca**: Vector DB indisponível\n2. **Falha no embedding**: API de embedding falha\n3. **Falha no LLM**: API do modelo falha\n4. **Timeout**: Requisições muito lentas\n5. **Rate limiting**: Muitas requisições\n\n### Implementação de Retry com Exponential Backoff\n\n```javascript\nasync function retryWithBackoff(fn, maxRetries = 3, baseDelay = 1000) {\n  for (let attempt = 0; attempt < maxRetries; attempt++) {\n    try {\n      return await fn();\n    } catch (error) {\n      // Não retry em erros que não são temporários\n      if (error.status === 400 || error.status === 401) {\n        throw error;\n      }\n      \n      // Última tentativa, lançar erro\n      if (attempt === maxRetries - 1) {\n        throw error;\n      }\n      \n      // Exponential backoff\n      const delay = baseDelay * Math.pow(2, attempt);\n      const jitter = Math.random() * 1000; // Adicionar jitter\n      \n      await new Promise(resolve => setTimeout(resolve, delay + jitter));\n    }\n  }\n}\n\n// Uso\nconst result = await retryWithBackoff(\n  () => openai.chat.completions.create({...}),\n  3,\n  1000\n);\n```\n\n### Circuit Breaker Pattern\n\nEvita sobrecarregar serviços que estão falhando:\n\n```javascript\nclass CircuitBreaker {\n  constructor(threshold = 5, timeout = 60000) {\n    this.failureCount = 0;\n    this.threshold = threshold;\n    this.timeout = timeout;\n    this.state = 'CLOSED'; // CLOSED, OPEN, HALF_OPEN\n    this.nextAttempt = Date.now();\n  }\n  \n  async execute(fn) {\n    if (this.state === 'OPEN') {\n      if (Date.now() < this.nextAttempt) {\n        throw new Error('Circuit breaker is OPEN');\n      }\n      this.state = 'HALF_OPEN';\n    }\n    \n    try {\n      const result = await fn();\n      this.onSuccess();\n      return result;\n    } catch (error) {\n      this.onFailure();\n      throw error;\n    }\n  }\n  \n  onSuccess() {\n    this.failureCount = 0;\n    this.state = 'CLOSED';\n  }\n  \n  onFailure() {\n    this.failureCount++;\n    if (this.failureCount >= this.threshold) {\n      this.state = 'OPEN';\n      this.nextAttempt = Date.now() + this.timeout;\n    }\n  }\n}\n\n// Uso\nconst breaker = new CircuitBreaker(5, 60000);\n\ntry {\n  const result = await breaker.execute(\n    () => openai.chat.completions.create({...})\n  );\n} catch (error) {\n  // Fallback ou erro\n}\n```\n\n### Fallback Strategies\n\nQuando o sistema principal falha, use fallbacks:\n\n```javascript\nclass RAGSystemWithFallback {\n  constructor() {\n    this.primaryLLM = new ChatOpenAI({ modelName: 'gpt-4' });\n    this.fallbackLLM = new ChatOpenAI({ modelName: 'gpt-3.5-turbo' });\n    this.cache = new Map();\n  }\n  \n  async query(userQuestion) {\n    // 1. Verificar cache\n    const cached = this.cache.get(userQuestion);\n    if (cached) return cached;\n    \n    try {\n      // 2. Tentar com modelo principal\n      const result = await this.primaryLLM.call(userQuestion);\n      this.cache.set(userQuestion, result);\n      return result;\n    } catch (error) {\n      console.warn('Primary LLM failed, using fallback', error);\n      \n      try {\n        // 3. Fallback para modelo secundário\n        const result = await this.fallbackLLM.call(userQuestion);\n        return result;\n      } catch (fallbackError) {\n        // 4. Fallback para resposta estática\n        return this.getStaticResponse(userQuestion);\n      }\n    }\n  }\n  \n  getStaticResponse(question) {\n    // Respostas pré-definidas para casos críticos\n    return 'Desculpe, não consigo processar sua pergunta no momento. Por favor, tente novamente mais tarde.';\n  }\n}\n```\n\n### Timeout Handling\n\n```javascript\nfunction withTimeout(promise, timeoutMs) {\n  return Promise.race([\n    promise,\n    new Promise((_, reject) =>\n      setTimeout(() => reject(new Error('Timeout')), timeoutMs)\n    )\n  ]);\n}\n\n// Uso\nconst result = await withTimeout(\n  openai.chat.completions.create({...}),\n  30000 // 30 segundos\n);\n```\n\n## Observabilidade e Logging\n\n### Estrutura de Logging\n\n```javascript\nclass RAGLogger {\n  constructor() {\n    this.logs = [];\n  }\n  \n  log(level, message, metadata = {}) {\n    const logEntry = {\n      timestamp: new Date().toISOString(),\n      level,\n      message,\n      ...metadata\n    };\n    \n    this.logs.push(logEntry);\n    \n    // Enviar para serviço de logging (ex: Datadog, Sentry)\n    this.sendToService(logEntry);\n  }\n  \n  info(message, metadata) {\n    this.log('INFO', message, metadata);\n  }\n  \n  error(message, error, metadata) {\n    this.log('ERROR', message, {\n      ...metadata,\n      error: {\n        message: error.message,\n        stack: error.stack\n      }\n    });\n  }\n  \n  async sendToService(logEntry) {\n    // Implementar envio para serviço de logging\n    // Ex: fetch('https://logs.example.com', { method: 'POST', body: JSON.stringify(logEntry) })\n  }\n}\n```\n\n### Instrumentação de RAG\n\n```javascript\nclass InstrumentedRAG {\n  constructor(ragSystem, logger) {\n    this.ragSystem = ragSystem;\n    this.logger = logger;\n  }\n  \n  async query(userQuestion) {\n    const startTime = Date.now();\n    const traceId = this.generateTraceId();\n    \n    this.logger.info('RAG query started', {\n      traceId,\n      question: userQuestion\n    });\n    \n    try {\n      // 1. Busca\n      const searchStart = Date.now();\n      const relevantDocs = await this.ragSystem.search(userQuestion);\n      const searchDuration = Date.now() - searchStart;\n      \n      this.logger.info('Search completed', {\n        traceId,\n        duration: searchDuration,\n        docCount: relevantDocs.length\n      });\n      \n      // 2. Geração\n      const generationStart = Date.now();\n      const answer = await this.ragSystem.generate(userQuestion, relevantDocs);\n      const generationDuration = Date.now() - generationStart;\n      \n      const totalDuration = Date.now() - startTime;\n      \n      this.logger.info('RAG query completed', {\n        traceId,\n        totalDuration,\n        searchDuration,\n        generationDuration,\n        answerLength: answer.length\n      });\n      \n      return answer;\n    } catch (error) {\n      this.logger.error('RAG query failed', error, {\n        traceId,\n        duration: Date.now() - startTime\n      });\n      throw error;\n    }\n  }\n  \n  generateTraceId() {\n    return `trace-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n```\n\n### Métricas Importantes\n\n```javascript\nclass RAGMetrics {\n  constructor() {\n    this.metrics = {\n      totalQueries: 0,\n      successfulQueries: 0,\n      failedQueries: 0,\n      averageLatency: 0,\n      averageSearchLatency: 0,\n      averageGenerationLatency: 0,\n      cacheHitRate: 0,\n      averageTokensUsed: 0\n    };\n  }\n  \n  recordQuery(success, latency, searchLatency, generationLatency, tokensUsed, cacheHit) {\n    this.metrics.totalQueries++;\n    \n    if (success) {\n      this.metrics.successfulQueries++;\n    } else {\n      this.metrics.failedQueries++;\n    }\n    \n    // Calcular médias\n    this.metrics.averageLatency = \n      (this.metrics.averageLatency * (this.metrics.totalQueries - 1) + latency) / \n      this.metrics.totalQueries;\n    \n    this.metrics.averageSearchLatency = \n      (this.metrics.averageSearchLatency * (this.metrics.totalQueries - 1) + searchLatency) / \n      this.metrics.totalQueries;\n    \n    this.metrics.averageGenerationLatency = \n      (this.metrics.averageGenerationLatency * (this.metrics.totalQueries - 1) + generationLatency) / \n      this.metrics.totalQueries;\n    \n    this.metrics.averageTokensUsed = \n      (this.metrics.averageTokensUsed * (this.metrics.totalQueries - 1) + tokensUsed) / \n      this.metrics.totalQueries;\n    \n    if (cacheHit) {\n      this.metrics.cacheHitRate = \n        (this.metrics.cacheHitRate * (this.metrics.totalQueries - 1) + 1) / \n        this.metrics.totalQueries;\n    }\n  }\n  \n  getMetrics() {\n    return {\n      ...this.metrics,\n      successRate: this.metrics.successfulQueries / this.metrics.totalQueries,\n      failureRate: this.metrics.failedQueries / this.metrics.totalQueries\n    };\n  }\n}\n```\n\n## Depuração de Sistemas RAG\n\n### Debug Mode\n\n```javascript\nclass DebuggableRAG {\n  constructor(ragSystem, debug = false) {\n    this.ragSystem = ragSystem;\n    this.debug = debug;\n  }\n  \n  async query(userQuestion) {\n    const debugInfo = {\n      question: userQuestion,\n      steps: []\n    };\n    \n    // Step 1: Embedding\n    const embeddingStart = Date.now();\n    const queryEmbedding = await this.ragSystem.createEmbedding(userQuestion);\n    debugInfo.steps.push({\n      step: 'embedding',\n      duration: Date.now() - embeddingStart,\n      embedding: this.debug ? queryEmbedding.slice(0, 5) : '[hidden]'\n    });\n    \n    // Step 2: Search\n    const searchStart = Date.now();\n    const results = await this.ragSystem.search(queryEmbedding);\n    debugInfo.steps.push({\n      step: 'search',\n      duration: Date.now() - searchStart,\n      resultsCount: results.length,\n      topResults: this.debug ? results.slice(0, 3).map(r => ({\n        content: r.content.substring(0, 100),\n        similarity: r.similarity\n      })) : '[hidden]'\n    });\n    \n    // Step 3: Generation\n    const generationStart = Date.now();\n    const answer = await this.ragSystem.generate(userQuestion, results);\n    debugInfo.steps.push({\n      step: 'generation',\n      duration: Date.now() - generationStart,\n      answerLength: answer.length\n    });\n    \n    if (this.debug) {\n      console.log('RAG Debug Info:', JSON.stringify(debugInfo, null, 2));\n    }\n    \n    return { answer, debugInfo };\n  }\n}\n```\n\n### Visualização de RAG Pipeline\n\n```javascript\nfunction visualizeRAGPipeline(debugInfo) {\n  console.log('\\n=== RAG Pipeline Visualization ===\\n');\n  \n  debugInfo.steps.forEach((step, index) => {\n    console.log(`${index + 1}. ${step.step.toUpperCase()}`);\n    console.log(`   Duration: ${step.duration}ms`);\n    \n    if (step.resultsCount) {\n      console.log(`   Results: ${step.resultsCount}`);\n    }\n    \n    if (step.topResults) {\n      console.log('   Top Results:');\n      step.topResults.forEach((result, i) => {\n        console.log(`     ${i + 1}. Similarity: ${result.similarity.toFixed(3)}`);\n        console.log(`        Content: ${result.content}...`);\n      });\n    }\n    \n    console.log('');\n  });\n  \n  const totalDuration = debugInfo.steps.reduce((sum, step) => sum + step.duration, 0);\n  console.log(`Total Duration: ${totalDuration}ms\\n`);\n}\n```\n\n## Exemplo Completo: Sistema RAG Profissional\n\n```javascript\nclass ProductionRAGSystem {\n  constructor() {\n    this.llm = new ChatOpenAI({ modelName: 'gpt-4' });\n    this.vectorStore = new VectorStore();\n    this.cache = new ResponseCache();\n    this.logger = new RAGLogger();\n    this.metrics = new RAGMetrics();\n    this.circuitBreaker = new CircuitBreaker();\n  }\n  \n  async query(userQuestion) {\n    const startTime = Date.now();\n    const traceId = this.generateTraceId();\n    \n    try {\n      // 1. Verificar cache\n      const cached = await this.cache.get(userQuestion);\n      if (cached) {\n        this.metrics.recordQuery(true, Date.now() - startTime, 0, 0, 0, true);\n        return cached;\n      }\n      \n      // 2. Buscar documentos (com retry)\n      const searchStart = Date.now();\n      const relevantDocs = await retryWithBackoff(\n        () => this.vectorStore.similaritySearch(userQuestion, 5),\n        3\n      );\n      const searchDuration = Date.now() - searchStart;\n      \n      // 3. Gerar resposta (com circuit breaker)\n      const generationStart = Date.now();\n      const answer = await this.circuitBreaker.execute(\n        () => this.generateAnswer(userQuestion, relevantDocs)\n      );\n      const generationDuration = Date.now() - generationStart;\n      \n      // 4. Cachear resposta\n      await this.cache.set(userQuestion, answer);\n      \n      // 5. Registrar métricas\n      const totalDuration = Date.now() - startTime;\n      this.metrics.recordQuery(\n        true,\n        totalDuration,\n        searchDuration,\n        generationDuration,\n        answer.length / 4, // Aproximação de tokens\n        false\n      );\n      \n      // 6. Log\n      this.logger.info('Query successful', {\n        traceId,\n        duration: totalDuration,\n        question: userQuestion.substring(0, 100)\n      });\n      \n      return answer;\n    } catch (error) {\n      this.metrics.recordQuery(false, Date.now() - startTime, 0, 0, 0, false);\n      this.logger.error('Query failed', error, { traceId });\n      throw error;\n    }\n  }\n  \n  async generateAnswer(question, docs) {\n    const context = docs.map(d => d.content).join('\\n\\n');\n    \n    const response = await this.llm.call([\n      {\n        role: 'system',\n        content: `Responda baseado no contexto:\\n\\n${context}`\n      },\n      {\n        role: 'user',\n        content: question\n      }\n    ]);\n    \n    return response.content;\n  }\n  \n  generateTraceId() {\n    return `trace-${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n  }\n}\n```\n\n## Conclusão\n\nConstruir sistemas RAG em produção requer muito mais que apenas buscar documentos e gerar respostas. É necessário:\n\n**Orquestração**:\n- Usar ferramentas como LangChain ou Vercel AI SDK\n- Gerenciar fluxos complexos\n- Integrar múltiplos componentes\n\n**Confiabilidade**:\n- Implementar retry com exponential backoff\n- Usar circuit breakers\n- Ter estratégias de fallback\n- Lidar com timeouts\n\n**Observabilidade**:\n- Logging estruturado\n- Métricas detalhadas\n- Tracing de requisições\n- Debug mode para desenvolvimento\n\n**Otimização**:\n- Cache de respostas\n- Otimização de busca\n- Monitoramento de performance\n\nNo próximo tópico, vamos colocar a mão na massa e integrar um LLM a um back-end existente, aplicando todas essas técnicas na prática.",
  "resources": [
    {
      "type": "link",
      "title": "LangChain Documentation",
      "url": "https://python.langchain.com",
      "description": "Documentação completa do LangChain"
    },
    {
      "type": "link",
      "title": "LangChain.js",
      "url": "https://js.langchain.com",
      "description": "Versão JavaScript do LangChain"
    },
    {
      "type": "link",
      "title": "Vercel AI SDK",
      "url": "https://sdk.vercel.ai",
      "description": "Documentação do Vercel AI SDK"
    },
    {
      "type": "code",
      "title": "Circuit Breaker Pattern",
      "description": "Implementações e exemplos do padrão Circuit Breaker"
    },
    {
      "type": "link",
      "title": "Observability for AI Systems",
      "description": "Guia de observabilidade para sistemas de IA"
    }
  ]
}

