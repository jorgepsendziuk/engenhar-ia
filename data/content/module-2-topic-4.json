{
  "id": "module-2-topic-4",
  "title": "Boas práticas de consistência e custo-eficiência",
  "content": "## Introdução\n\nAo construir aplicações de IA em produção, dois fatores críticos determinam o sucesso: **consistência** (garantir que o modelo se comporte de forma previsível) e **custo-eficiência** (maximizar valor enquanto minimiza gastos). Este tópico explora estratégias práticas para otimizar ambos os aspectos.\n\nAPIs de IA podem se tornar caras rapidamente, especialmente em escala. Um sistema que processa milhares de requisições por dia pode facilmente gerar custos de centenas ou milhares de dólares mensais se não for otimizado. Da mesma forma, inconsistências no comportamento do modelo podem levar a experiências ruins do usuário e problemas de confiabilidade.\n\n## Entendendo Custos de APIs de IA\n\n### Como Funciona o Pricing\n\nA maioria das APIs de IA cobra baseado em **tokens**, que são unidades de texto processadas:\n\n- **Tokens de entrada (input)**: Texto que você envia para o modelo\n- **Tokens de saída (output)**: Texto que o modelo gera\n- **Pricing**: Geralmente mais caro para tokens de saída\n\n### Exemplo de Cálculo de Custo\n\n```javascript\n// Exemplo: GPT-4 Turbo\n// Input: $10 por 1M tokens\n// Output: $30 por 1M tokens\n\nfunction calculateCost(inputTokens, outputTokens) {\n  const inputCost = (inputTokens / 1_000_000) * 10;\n  const outputCost = (outputTokens / 1_000_000) * 30;\n  return inputCost + outputCost;\n}\n\n// Uma requisição com 1000 tokens de entrada e 500 tokens de saída\nconst cost = calculateCost(1000, 500);\nconsole.log(`Custo: $${cost.toFixed(4)}`); // ~$0.025\n\n// Em escala: 10.000 requisições/dia = $250/dia = $7.500/mês\n```\n\n### Fatores que Afetam Custos\n\n1. **Modelo escolhido**: GPT-4 é muito mais caro que GPT-3.5\n2. **Tamanho do contexto**: Mais tokens = mais custo\n3. **Tamanho da resposta**: Respostas longas custam mais\n4. **Frequência de requisições**: Volume impacta diretamente\n5. **Retries e fallbacks**: Requisições repetidas aumentam custos\n\n## Estratégias de Redução de Custos\n\n### 1. Escolher o Modelo Certo para Cada Tarefa\n\nNem toda tarefa precisa do modelo mais avançado:\n\n```javascript\nclass ModelSelector {\n  selectModel(task) {\n    // Tarefas simples: modelo barato\n    if (task.complexity === 'simple') {\n      return 'gpt-3.5-turbo'; // $0.50/$1.50 por 1M tokens\n    }\n    \n    // Tarefas moderadas: modelo médio\n    if (task.complexity === 'moderate') {\n      return 'gpt-4-turbo'; // $10/$30 por 1M tokens\n    }\n    \n    // Tarefas complexas: modelo avançado\n    if (task.complexity === 'complex') {\n      return 'gpt-4'; // $30/$60 por 1M tokens\n    }\n  }\n  \n  async processWithFallback(prompt, maxRetries = 2) {\n    // Tenta primeiro com modelo barato\n    try {\n      return await this.callModel('gpt-3.5-turbo', prompt);\n    } catch (error) {\n      // Se falhar, tenta com modelo mais avançado\n      if (maxRetries > 0) {\n        return await this.callModel('gpt-4-turbo', prompt);\n      }\n      throw error;\n    }\n  }\n}\n```\n\n### 2. Reduzir Tamanho do Contexto\n\nEnvie apenas o contexto necessário:\n\n```javascript\n// ❌ Ruim: Enviando contexto desnecessário\nconst badPrompt = `\nHistórico completo da conversa (5000 tokens)...\n\nPergunta atual: Qual é a capital do Brasil?\n`;\n\n// ✅ Bom: Apenas contexto relevante\nfunction buildEfficientPrompt(question, relevantHistory) {\n  // Extrair apenas últimas 3 mensagens relevantes\n  const recentContext = relevantHistory.slice(-3);\n  \n  return `\nContexto relevante:\n${recentContext.map(m => `${m.role}: ${m.content}`).join('\\n')}\n\nPergunta: ${question}\n`;\n}\n```\n\n### 3. Limitar Tamanho de Respostas\n\nUse `max_tokens` para controlar o tamanho:\n\n```javascript\nasync function generateConciseResponse(prompt) {\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4-turbo',\n    messages: [{ role: 'user', content: prompt }],\n    max_tokens: 150, // Limita resposta a ~150 tokens\n    temperature: 0.7\n  });\n  \n  return response.choices[0].message.content;\n}\n\n// Para respostas que precisam ser curtas\nconst shortPrompt = `${prompt}\\n\\nIMPORTANTE: Responda de forma concisa, máximo 2-3 frases.`;\n```\n\n### 4. Usar Streaming para Respostas Longas\n\nStreaming permite processar enquanto recebe, mas também pode reduzir custos se você cancelar cedo:\n\n```javascript\nasync function streamWithEarlyStop(prompt, maxTokens = 500) {\n  const stream = await openai.chat.completions.create({\n    model: 'gpt-4-turbo',\n    messages: [{ role: 'user', content: prompt }],\n    stream: true,\n    max_tokens: maxTokens\n  });\n  \n  let fullResponse = '';\n  let tokenCount = 0;\n  \n  for await (const chunk of stream) {\n    const content = chunk.choices[0]?.delta?.content || '';\n    fullResponse += content;\n    tokenCount++;\n    \n    // Parar se atingir limite ou encontrar marcador de fim\n    if (tokenCount >= maxTokens || fullResponse.includes('[FIM]')) {\n      break;\n    }\n  }\n  \n  return fullResponse;\n}\n```\n\n## Cache de Respostas e Embeddings\n\n### Cache de Respostas Completas\n\nCache respostas idênticas ou muito similares:\n\n```javascript\nconst NodeCache = require('node-cache');\nconst crypto = require('crypto');\n\nclass ResponseCache {\n  constructor(ttl = 3600) { // 1 hora default\n    this.cache = new NodeCache({ stdTTL: ttl });\n  }\n  \n  getCacheKey(prompt, model, temperature) {\n    const content = `${prompt}:${model}:${temperature}`;\n    return crypto.createHash('sha256').update(content).digest('hex');\n  }\n  \n  async getOrGenerate(prompt, model, generateFn) {\n    const key = this.getCacheKey(prompt, model, 0); // temperature 0 para cache determinístico\n    \n    // Verificar cache\n    const cached = this.cache.get(key);\n    if (cached) {\n      console.log('Cache hit!');\n      return cached;\n    }\n    \n    // Gerar e cachear\n    console.log('Cache miss, gerando...');\n    const response = await generateFn(prompt, model);\n    this.cache.set(key, response);\n    \n    return response;\n  }\n}\n\n// Uso\nconst cache = new ResponseCache(3600); // Cache por 1 hora\n\nasync function getCachedResponse(prompt) {\n  return await cache.getOrGenerate(\n    prompt,\n    'gpt-4-turbo',\n    async (p, m) => {\n      const response = await openai.chat.completions.create({\n        model: m,\n        messages: [{ role: 'user', content: p }],\n        temperature: 0 // Determinístico para cache\n      });\n      return response.choices[0].message.content;\n    }\n  );\n}\n```\n\n### Cache Semântico com Embeddings\n\nPara perguntas similares (não idênticas), use cache semântico:\n\n```javascript\nconst { OpenAI } = require('openai');\n\nclass SemanticCache {\n  constructor(openai, similarityThreshold = 0.95) {\n    this.openai = openai;\n    this.cache = new Map();\n    this.threshold = similarityThreshold;\n  }\n  \n  async getEmbedding(text) {\n    const response = await this.openai.embeddings.create({\n      model: 'text-embedding-3-small',\n      input: text\n    });\n    return response.data[0].embedding;\n  }\n  \n  cosineSimilarity(vecA, vecB) {\n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n    \n    for (let i = 0; i < vecA.length; i++) {\n      dotProduct += vecA[i] * vecB[i];\n      normA += vecA[i] * vecA[i];\n      normB += vecB[i] * vecB[i];\n    }\n    \n    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n  }\n  \n  async findSimilar(prompt) {\n    const promptEmbedding = await this.getEmbedding(prompt);\n    \n    for (const [cachedPrompt, cachedResponse] of this.cache.entries()) {\n      const cachedEmbedding = await this.getEmbedding(cachedPrompt);\n      const similarity = this.cosineSimilarity(promptEmbedding, cachedEmbedding);\n      \n      if (similarity >= this.threshold) {\n        return cachedResponse;\n      }\n    }\n    \n    return null;\n  }\n  \n  async getOrGenerate(prompt, generateFn) {\n    // Verificar cache semântico\n    const similar = await this.findSimilar(prompt);\n    if (similar) {\n      console.log('Cache semântico hit!');\n      return similar;\n    }\n    \n    // Gerar e cachear\n    const response = await generateFn(prompt);\n    this.cache.set(prompt, response);\n    \n    return response;\n  }\n}\n```\n\n### Cache de Embeddings\n\nEmbeddings são caros de calcular. Cacheie-os:\n\n```javascript\nclass EmbeddingCache {\n  constructor() {\n    this.cache = new Map();\n  }\n  \n  async getEmbedding(text, openai) {\n    // Verificar cache\n    if (this.cache.has(text)) {\n      return this.cache.get(text);\n    }\n    \n    // Calcular e cachear\n    const response = await openai.embeddings.create({\n      model: 'text-embedding-3-small',\n      input: text\n    });\n    \n    const embedding = response.data[0].embedding;\n    this.cache.set(text, embedding);\n    \n    return embedding;\n  }\n}\n```\n\n## Reuso de Contexto\n\n### Manter Contexto Entre Requisições\n\nEm conversas, reutilize o contexto de forma eficiente:\n\n```javascript\nclass ConversationManager {\n  constructor(maxContextTokens = 4000) {\n    this.maxContextTokens = maxContextTokens;\n    this.messageHistory = [];\n  }\n  \n  addMessage(role, content) {\n    this.messageHistory.push({ role, content });\n    this.trimContext();\n  }\n  \n  trimContext() {\n    // Estimar tokens (aproximação: 1 token ≈ 4 caracteres)\n    let totalChars = this.messageHistory.reduce(\n      (sum, msg) => sum + msg.content.length, 0\n    );\n    \n    const estimatedTokens = totalChars / 4;\n    \n    // Se exceder limite, remover mensagens mais antigas (exceto system)\n    if (estimatedTokens > this.maxContextTokens) {\n      const systemMessages = this.messageHistory.filter(m => m.role === 'system');\n      const otherMessages = this.messageHistory.filter(m => m.role !== 'system');\n      \n      // Manter system + últimas N mensagens\n      while (estimatedTokens > this.maxContextTokens && otherMessages.length > 2) {\n        otherMessages.shift(); // Remove mais antiga\n        totalChars = otherMessages.reduce((sum, msg) => sum + msg.content.length, 0) +\n                    systemMessages.reduce((sum, msg) => sum + msg.content.length, 0);\n      }\n      \n      this.messageHistory = [...systemMessages, ...otherMessages];\n    }\n  }\n  \n  getMessages() {\n    return this.messageHistory;\n  }\n}\n```\n\n### Resumir Contexto Antigo\n\nEm vez de manter todo o histórico, resuma partes antigas:\n\n```javascript\nasync function summarizeOldContext(oldMessages, openai) {\n  const oldContext = oldMessages\n    .map(m => `${m.role}: ${m.content}`)\n    .join('\\n');\n  \n  const summaryPrompt = `Resuma o seguinte contexto de conversa em 2-3 frases, mantendo informações importantes:\n\n${oldContext}`;\n  \n  const summary = await openai.chat.completions.create({\n    model: 'gpt-3.5-turbo', // Modelo barato para resumo\n    messages: [{ role: 'user', content: summaryPrompt }],\n    max_tokens: 100\n  });\n  \n  return summary.choices[0].message.content;\n}\n\n// Uso\nif (conversationHistory.length > 10) {\n  const oldMessages = conversationHistory.slice(0, -5); // Primeiras mensagens\n  const summary = await summarizeOldContext(oldMessages, openai);\n  \n  // Substituir mensagens antigas por resumo\n  conversationHistory = [\n    { role: 'system', content: `Contexto anterior: ${summary}` },\n    ...conversationHistory.slice(-5) // Últimas 5 mensagens\n  ];\n}\n```\n\n## Redução de Tokens\n\n### Técnicas de Compressão de Prompt\n\n```javascript\nfunction compressPrompt(originalPrompt) {\n  // Remover espaços extras\n  let compressed = originalPrompt.replace(/\\s+/g, ' ').trim();\n  \n  // Remover comentários se houver código\n  compressed = compressed.replace(/\\/\\/.*$/gm, '');\n  compressed = compressed.replace(/\\/\\*[\\s\\S]*?\\*\\//g, '');\n  \n  // Abreviar instruções comuns\n  const abbreviations = {\n    'por favor': 'pf',\n    'exemplo': 'ex',\n    'importante': 'imp',\n    // ... mais abreviações\n  };\n  \n  for (const [full, abbrev] of Object.entries(abbreviations)) {\n    compressed = compressed.replace(new RegExp(full, 'gi'), abbrev);\n  }\n  \n  return compressed;\n}\n```\n\n### Usar Formato Estruturado\n\nFormato estruturado pode ser mais eficiente:\n\n```javascript\n// ❌ Verboso\nconst verbosePrompt = `\nPor favor, analise o seguinte código JavaScript e forneça:\n1. Uma lista de problemas encontrados\n2. Sugestões de melhoria\n3. Exemplos de código corrigido\n\nCódigo a analisar:\n[100 linhas de código]\n`;\n\n// ✅ Conciso e estruturado\nconst concisePrompt = `\nAnalise código JS. Retorne JSON:\n{\n  \"problemas\": [\"...\"],\n  \"sugestões\": [\"...\"],\n  \"exemplos\": [\"...\"]\n}\n\nCódigo:\n[código]\n`;\n```\n\n## Padrões para Consistência\n\n### 1. Temperature e Top-p Consistentes\n\nUse configurações consistentes para resultados previsíveis:\n\n```javascript\nconst CONSISTENT_CONFIG = {\n  factual: {\n    temperature: 0.1,\n    top_p: 0.9\n  },\n  creative: {\n    temperature: 0.7,\n    top_p: 0.95\n  },\n  balanced: {\n    temperature: 0.5,\n    top_p: 0.9\n  }\n};\n\nfunction getConfigForTask(taskType) {\n  return CONSISTENT_CONFIG[taskType] || CONSISTENT_CONFIG.balanced;\n}\n```\n\n### 2. System Messages Consistentes\n\nDefina personas e comportamentos de forma consistente:\n\n```javascript\nconst SYSTEM_MESSAGES = {\n  codeReviewer: `Você é um code reviewer sênior. Sempre:\n- Forneça feedback construtivo\n- Cite linhas de código específicas\n- Sugira melhorias práticas\n- Use formato markdown`,\n  \n  translator: `Você é um tradutor profissional. Sempre:\n- Mantenha o tom original\n- Preserve termos técnicos quando apropriado\n- Indique incertezas quando necessário`,\n  \n  analyst: `Você é um analista de dados. Sempre:\n- Forneça dados quantitativos\n- Cite fontes\n- Indique limitações\n- Use visualizações quando útil`\n};\n```\n\n### 3. Validação de Respostas\n\nValide respostas para garantir consistência:\n\n```javascript\nclass ResponseValidator {\n  validateFormat(response, expectedFormat) {\n    if (expectedFormat === 'json') {\n      try {\n        JSON.parse(response);\n        return true;\n      } catch {\n        return false;\n      }\n    }\n    \n    if (expectedFormat === 'list') {\n      // Verificar se é uma lista\n      return /^[-*]\\s/.test(response) || /^\\d+\\.\\s/.test(response);\n    }\n    \n    return true;\n  }\n  \n  validateContent(response, requirements) {\n    const checks = [];\n    \n    if (requirements.minLength) {\n      checks.push(response.length >= requirements.minLength);\n    }\n    \n    if (requirements.maxLength) {\n      checks.push(response.length <= requirements.maxLength);\n    }\n    \n    if (requirements.requiredKeywords) {\n      const hasKeywords = requirements.requiredKeywords.every(\n        keyword => response.toLowerCase().includes(keyword.toLowerCase())\n      );\n      checks.push(hasKeywords);\n    }\n    \n    return checks.every(check => check === true);\n  }\n  \n  async validateAndRetry(prompt, generateFn, requirements, maxRetries = 2) {\n    for (let i = 0; i < maxRetries; i++) {\n      const response = await generateFn(prompt);\n      \n      if (this.validateFormat(response, requirements.format) &&\n          this.validateContent(response, requirements)) {\n        return response;\n      }\n      \n      // Adicionar instruções mais específicas na retry\n      if (i < maxRetries - 1) {\n        prompt += `\\n\\nIMPORTANTE: ${JSON.stringify(requirements)}`;\n      }\n    }\n    \n    throw new Error('Validação falhou após múltiplas tentativas');\n  }\n}\n```\n\n### 4. Seed para Reproduzibilidade\n\nUse `seed` para resultados determinísticos quando necessário:\n\n```javascript\nasync function generateDeterministic(prompt, seed = 42) {\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4-turbo',\n    messages: [{ role: 'user', content: prompt }],\n    temperature: 0, // Deve ser 0 para seed funcionar\n    seed: seed\n  });\n  \n  return response.choices[0].message.content;\n}\n\n// Mesmo prompt + mesmo seed = mesma resposta\nconst response1 = await generateDeterministic('Explique IA', 42);\nconst response2 = await generateDeterministic('Explique IA', 42);\n// response1 === response2\n```\n\n## Monitoramento e Otimização Contínua\n\n### Tracking de Custos\n\n```javascript\nclass CostTracker {\n  constructor() {\n    this.costs = [];\n    this.dailyLimit = 100; // $100/dia\n  }\n  \n  trackRequest(model, inputTokens, outputTokens, pricing) {\n    const inputCost = (inputTokens / 1_000_000) * pricing.input;\n    const outputCost = (outputTokens / 1_000_000) * pricing.output;\n    const totalCost = inputCost + outputCost;\n    \n    this.costs.push({\n      timestamp: new Date(),\n      model,\n      inputTokens,\n      outputTokens,\n      cost: totalCost\n    });\n    \n    return totalCost;\n  }\n  \n  getDailyCost() {\n    const today = new Date().toDateString();\n    return this.costs\n      .filter(c => new Date(c.timestamp).toDateString() === today)\n      .reduce((sum, c) => sum + c.cost, 0);\n  }\n  \n  checkLimit() {\n    return this.getDailyCost() < this.dailyLimit;\n  }\n}\n```\n\n### Análise de Uso\n\n```javascript\nclass UsageAnalyzer {\n  analyzeUsage(costTracker) {\n    const analysis = {\n      totalRequests: costTracker.costs.length,\n      totalCost: costTracker.costs.reduce((sum, c) => sum + c.cost, 0),\n      averageCostPerRequest: 0,\n      mostExpensiveModel: null,\n      costByModel: {}\n    };\n    \n    if (analysis.totalRequests > 0) {\n      analysis.averageCostPerRequest = analysis.totalCost / analysis.totalRequests;\n    }\n    \n    // Agrupar por modelo\n    costTracker.costs.forEach(c => {\n      if (!analysis.costByModel[c.model]) {\n        analysis.costByModel[c.model] = { count: 0, totalCost: 0 };\n      }\n      analysis.costByModel[c.model].count++;\n      analysis.costByModel[c.model].totalCost += c.cost;\n    });\n    \n    // Encontrar modelo mais caro\n    let maxCost = 0;\n    for (const [model, data] of Object.entries(analysis.costByModel)) {\n      if (data.totalCost > maxCost) {\n        maxCost = data.totalCost;\n        analysis.mostExpensiveModel = model;\n      }\n    }\n    \n    return analysis;\n  }\n}\n```\n\n## Exemplo Prático: Sistema Otimizado\n\n```javascript\nclass OptimizedAISystem {\n  constructor(openai) {\n    this.openai = openai;\n    this.cache = new ResponseCache(3600);\n    this.costTracker = new CostTracker();\n    this.conversationManager = new ConversationManager(4000);\n  }\n  \n  async processRequest(userPrompt, options = {}) {\n    // 1. Verificar cache\n    const cached = await this.cache.getOrGenerate(\n      userPrompt,\n      'gpt-4-turbo',\n      async (prompt, model) => {\n        // 2. Selecionar modelo apropriado\n        const selectedModel = this.selectModel(userPrompt, options);\n        \n        // 3. Construir prompt eficiente\n        const efficientPrompt = this.buildEfficientPrompt(userPrompt, options);\n        \n        // 4. Fazer requisição\n        const response = await this.openai.chat.completions.create({\n          model: selectedModel,\n          messages: this.conversationManager.getMessages().concat([\n            { role: 'user', content: efficientPrompt }\n          ]),\n          max_tokens: options.maxTokens || 500,\n          temperature: options.temperature || 0.7\n        });\n        \n        // 5. Trackear custos\n        const usage = response.usage;\n        this.costTracker.trackRequest(\n          selectedModel,\n          usage.prompt_tokens,\n          usage.completion_tokens,\n          this.getPricing(selectedModel)\n        );\n        \n        // 6. Adicionar ao histórico\n        this.conversationManager.addMessage('user', userPrompt);\n        this.conversationManager.addMessage('assistant', response.choices[0].message.content);\n        \n        return response.choices[0].message.content;\n      }\n    );\n    \n    return cached;\n  }\n  \n  selectModel(prompt, options) {\n    if (options.forceModel) return options.forceModel;\n    \n    // Lógica de seleção baseada em complexidade\n    const complexity = this.estimateComplexity(prompt);\n    \n    if (complexity === 'simple') return 'gpt-3.5-turbo';\n    if (complexity === 'moderate') return 'gpt-4-turbo';\n    return 'gpt-4';\n  }\n  \n  buildEfficientPrompt(prompt, options) {\n    // Comprimir e otimizar prompt\n    let optimized = prompt.trim();\n    \n    // Adicionar instruções de formato se necessário\n    if (options.format) {\n      optimized += `\\n\\nFormato de resposta: ${options.format}`;\n    }\n    \n    // Adicionar limite de tamanho\n    if (options.concise) {\n      optimized += '\\n\\nIMPORTANTE: Seja conciso.';\n    }\n    \n    return optimized;\n  }\n  \n  getPricing(model) {\n    const pricing = {\n      'gpt-3.5-turbo': { input: 0.5, output: 1.5 },\n      'gpt-4-turbo': { input: 10, output: 30 },\n      'gpt-4': { input: 30, output: 60 }\n    };\n    return pricing[model] || pricing['gpt-4-turbo'];\n  }\n}\n```\n\n## Conclusão\n\nOtimizar custos e garantir consistência são aspectos fundamentais de aplicações de IA em produção. As estratégias apresentadas incluem:\n\n**Redução de Custos**:\n- Escolher modelos apropriados para cada tarefa\n- Implementar cache eficiente\n- Reduzir tamanho de contexto e respostas\n- Reutilizar contexto de forma inteligente\n\n**Consistência**:\n- Usar configurações consistentes (temperature, top_p)\n- Validar respostas\n- Manter system messages consistentes\n- Usar seed para reprodutibilidade quando necessário\n\n**Monitoramento**:\n- Trackear custos em tempo real\n- Analisar padrões de uso\n- Identificar oportunidades de otimização\n\nAplicar essas técnicas pode resultar em reduções significativas de custo (50-80% em muitos casos) enquanto melhora a consistência e confiabilidade do sistema. No próximo tópico, exploraremos RAG avançado, onde muitas dessas técnicas de otimização serão aplicadas em um contexto mais complexo.",
  "resources": [
    {
      "type": "link",
      "title": "OpenAI Pricing",
      "url": "https://openai.com/pricing",
      "description": "Página oficial de pricing da OpenAI"
    },
    {
      "type": "link",
      "title": "Anthropic Pricing",
      "url": "https://www.anthropic.com/pricing",
      "description": "Página oficial de pricing da Anthropic"
    },
    {
      "type": "link",
      "title": "Token Usage Best Practices - OpenAI",
      "url": "https://platform.openai.com/docs/guides/rate-limits",
      "description": "Boas práticas de uso de tokens e rate limits"
    },
    {
      "type": "code",
      "title": "LangChain Caching",
      "url": "https://python.langchain.com/docs/modules/model_io/llms/caching",
      "description": "Exemplos de implementação de cache com LangChain"
    },
    {
      "type": "link",
      "title": "Cost Optimization Strategies",
      "description": "Estratégias avançadas de otimização de custos em aplicações de IA"
    }
  ]
}

