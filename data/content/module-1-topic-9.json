{
  "id": "module-1-topic-9",
  "title": "RAG, embeddings e busca semântica",
  "content": "## Introdução\n\nRAG (Retrieval-Augmented Generation), embeddings e busca semântica são tecnologias fundamentais para criar sistemas de IA que realmente \"entendem\" e trabalham com conhecimento. Este tópico explora esses conceitos e como aplicá-los na prática.\n\n## O que é RAG (Retrieval-Augmented Generation) e por que ele é tão importante\n\n### O Problema com LLMs\n\nLLMs têm limitações:\n\n- **Conhecimento estático**: Treinados em dados históricos\n- **Alucinações**: Podem inventar informações\n- **Contexto limitado**: Não acessam informações externas em tempo real\n- **Custo**: Re-treinar para atualizar conhecimento é caro\n\n### A Solução: RAG\n\n**RAG (Retrieval-Augmented Generation)** combina:\n\n1. **Retrieval (Busca)**: Busca informações relevantes em uma base de conhecimento\n2. **Augmentation (Aumento)**: Adiciona essas informações ao contexto do LLM\n3. **Generation (Geração)**: LLM gera resposta usando informações recuperadas\n\n### Como RAG Funciona\n\n```\n1. Usuário faz pergunta\n2. Sistema busca documentos relevantes (usando embeddings)\n3. Documentos são adicionados ao contexto do LLM\n4. LLM gera resposta baseada nos documentos + conhecimento pré-treinado\n5. Resposta é mais precisa e atualizada\n```\n\n### Por que RAG é Importante\n\n- **Atualização fácil**: Adicione novos documentos sem re-treinar\n- **Precisão**: Respostas baseadas em fontes específicas\n- **Rastreabilidade**: Pode citar fontes\n- **Custo**: Mais barato que fine-tuning\n- **Flexibilidade**: Funciona com qualquer base de conhecimento\n\n## O que são embeddings e Vector Databases\n\n### Embeddings\n\n**Embeddings** são representações numéricas de texto (ou outros dados) como vetores em um espaço de alta dimensão.\n\n**Características**:\n\n- **Semântica preservada**: Textos similares têm embeddings próximos\n- **Operações matemáticas**: Podemos medir similaridade\n- **Dimensão fixa**: Cada embedding tem tamanho fixo (ex: 768, 1536 dimensões)\n\n**Exemplo**:\n```javascript\n// Textos similares têm embeddings próximos\nconst embedding1 = getEmbedding('gato');\nconst embedding2 = getEmbedding('felino');\nconst embedding3 = getEmbedding('carro');\n\n// Distância entre embedding1 e embedding2 < distância entre embedding1 e embedding3\n```\n\n### Como Gerar Embeddings\n\n**Usando APIs**:\n```javascript\n// OpenAI Embeddings\nconst response = await openai.embeddings.create({\n    model: 'text-embedding-3-small',\n    input: 'Texto para embed'\n});\n\nconst embedding = response.data[0].embedding;\n```\n\n**Usando bibliotecas locais**:\n```javascript\n// Usando TensorFlow.js\nconst model = await use.load();\nconst embedding = await model.embed('Texto aqui');\n```\n\n### Vector Databases\n\n**Vector Databases** são bancos de dados otimizados para armazenar e buscar embeddings.\n\n**Características**:\n\n- **Busca por similaridade**: Encontra vetores mais próximos\n- **Índices otimizados**: Busca rápida mesmo com milhões de vetores\n- **Metadados**: Armazena informações adicionais junto com embeddings\n\n**Exemplos**:\n\n- **Pinecone**: Cloud-native, gerenciado\n- **Weaviate**: Open-source, auto-hospedado\n- **Qdrant**: Performance alta, open-source\n- **Chroma**: Simples, ideal para protótipos\n- **PostgreSQL com pgvector**: Extensão para PostgreSQL\n\n## Conceito de similarity search (busca por \"significado\", não só por texto exato)\n\n### Busca Tradicional vs Busca Semântica\n\n**Busca Tradicional (Keyword-based)**:\n```\nQuery: \"carro rápido\"\nResultado: Documentos que contêm palavras \"carro\" E \"rápido\"\nProblema: Não encontra \"veículo veloz\" ou \"automóvel acelerado\"\n```\n\n**Busca Semântica (Similarity Search)**:\n```\nQuery: \"carro rápido\"\n1. Converte query para embedding\n2. Busca embeddings similares no banco\n3. Retorna documentos com significado similar\nResultado: Encontra \"veículo veloz\", \"automóvel acelerado\", etc.\n```\n\n### Como Funciona\n\n1. **Indexação**:\n   - Converte documentos para embeddings\n   - Armazena embeddings + documentos no vector DB\n\n2. **Busca**:\n   - Converte query para embedding\n   - Calcula similaridade (cosine similarity, dot product)\n   - Retorna documentos mais similares\n\n### Exemplo Prático\n\n```javascript\n// 1. Criar embeddings dos documentos\nconst documents = [\n    'Carros são veículos de transporte',\n    'Automóveis usam combustível',\n    'Bicicletas são ecológicas'\n];\n\nconst embeddings = await Promise.all(\n    documents.map(doc => createEmbedding(doc))\n);\n\n// 2. Armazenar no vector DB\nawait vectorDB.upsert({\n    vectors: embeddings,\n    documents: documents\n});\n\n// 3. Buscar\nconst query = 'veículo motorizado';\nconst queryEmbedding = await createEmbedding(query);\n\nconst results = await vectorDB.query({\n    vector: queryEmbedding,\n    topK: 2  // Top 2 resultados\n});\n\n// Resultados: 'Carros são veículos...' e 'Automóveis usam...'\n// (mesmo sem palavras exatas na query)\n```\n\n## Projeto: Criando o primeiro RAG com JavaScript e Postgres\n\n### Setup\n\n```bash\nnpm install @supabase/supabase-js openai pgvector\n```\n\n### 1. Configurar PostgreSQL com pgvector\n\n```sql\n-- Habilitar extensão\nCREATE EXTENSION IF NOT EXISTS vector;\n\n-- Criar tabela\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT NOT NULL,\n    embedding vector(1536),  -- Dimensão do embedding\n    metadata JSONB\n);\n\n-- Criar índice para busca rápida\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);\n```\n\n### 2. Criar Embeddings e Armazenar\n\n```javascript\nimport { createClient } from '@supabase/supabase-js';\nimport OpenAI from 'openai';\n\nconst supabase = createClient(SUPABASE_URL, SUPABASE_KEY);\nconst openai = new OpenAI({ apiKey: OPENAI_KEY });\n\nasync function indexDocument(content, metadata = {}) {\n    // Criar embedding\n    const response = await openai.embeddings.create({\n        model: 'text-embedding-3-small',\n        input: content\n    });\n    \n    const embedding = response.data[0].embedding;\n    \n    // Armazenar no banco\n    const { data, error } = await supabase\n        .from('documents')\n        .insert({\n            content,\n            embedding,\n            metadata\n        });\n    \n    return { data, error };\n}\n```\n\n### 3. Buscar Documentos Relevantes\n\n```javascript\nasync function searchSimilar(query, topK = 5) {\n    // Criar embedding da query\n    const response = await openai.embeddings.create({\n        model: 'text-embedding-3-small',\n        input: query\n    });\n    \n    const queryEmbedding = response.data[0].embedding;\n    \n    // Buscar no banco usando pgvector\n    const { data, error } = await supabase.rpc('match_documents', {\n        query_embedding: queryEmbedding,\n        match_threshold: 0.7,\n        match_count: topK\n    });\n    \n    return data;\n}\n```\n\n### 4. Função SQL para Busca\n\n```sql\nCREATE OR REPLACE FUNCTION match_documents(\n    query_embedding vector(1536),\n    match_threshold float,\n    match_count int\n)\nRETURNS TABLE (\n    id int,\n    content text,\n    similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    RETURN QUERY\n    SELECT\n        documents.id,\n        documents.content,\n        1 - (documents.embedding <=> query_embedding) as similarity\n    FROM documents\n    WHERE 1 - (documents.embedding <=> query_embedding) > match_threshold\n    ORDER BY documents.embedding <=> query_embedding\n    LIMIT match_count;\nEND;\n$$;\n```\n\n### 5. Implementar RAG Completo\n\n```javascript\nasync function ragQuery(userQuestion) {\n    // 1. Buscar documentos relevantes\n    const relevantDocs = await searchSimilar(userQuestion, 3);\n    \n    // 2. Construir contexto\n    const context = relevantDocs\n        .map(doc => doc.content)\n        .join('\\n\\n');\n    \n    // 3. Gerar resposta com LLM\n    const response = await openai.chat.completions.create({\n        model: 'gpt-4',\n        messages: [\n            {\n                role: 'system',\n                content: `Você é um assistente que responde perguntas baseado no contexto fornecido.\\n\\nContexto:\\n${context}`\n            },\n            {\n                role: 'user',\n                content: userQuestion\n            }\n        ],\n        temperature: 0.7\n    });\n    \n    return {\n        answer: response.choices[0].message.content,\n        sources: relevantDocs.map(doc => ({ id: doc.id, content: doc.content }))\n    };\n}\n```\n\n### 6. Usar o RAG\n\n```javascript\n// Indexar documentos\nawait indexDocument('JavaScript é uma linguagem de programação...');\nawait indexDocument('Python é popular para data science...');\n\n// Fazer pergunta\nconst result = await ragQuery('O que é JavaScript?');\nconsole.log('Resposta:', result.answer);\nconsole.log('Fontes:', result.sources);\n```\n\n## Otimizações e Melhorias\n\n### Chunking Inteligente\n\nDividir documentos grandes em chunks menores:\n\n```javascript\nfunction chunkText(text, chunkSize = 500, overlap = 50) {\n    const chunks = [];\n    let start = 0;\n    \n    while (start < text.length) {\n        const end = Math.min(start + chunkSize, text.length);\n        chunks.push(text.slice(start, end));\n        start = end - overlap;\n    }\n    \n    return chunks;\n}\n```\n\n### Re-ranking\n\nMelhorar resultados com re-ranking:\n\n```javascript\n// Buscar mais resultados inicialmente\nconst candidates = await searchSimilar(query, 20);\n\n// Re-ranquear usando modelo mais sofisticado\nconst reranked = await rerank(candidates, query);\n\n// Retornar top K\nreturn reranked.slice(0, 5);\n```\n\n### Filtros de Metadados\n\n```javascript\n// Buscar apenas em documentos de um tipo específico\nconst results = await vectorDB.query({\n    vector: queryEmbedding,\n    filter: { type: 'documentation', language: 'javascript' },\n    topK: 5\n});\n```\n\n## Casos de Uso\n\n- **Chatbots com conhecimento**: Suporte ao cliente com base de conhecimento\n- **Q&A sobre documentos**: Responder perguntas sobre documentação\n- **Busca semântica**: Encontrar conteúdo por significado\n- **Recomendações**: Recomendar conteúdo similar\n- **Análise de código**: Buscar código similar ou padrões\n\n## Conclusão\n\nRAG, embeddings e busca semântica são tecnologias poderosas que permitem criar sistemas de IA verdadeiramente inteligentes. Combinando recuperação de informações relevantes com geração de texto, RAG supera muitas limitações dos LLMs puros.\n\nNo próximo tópico, exploraremos modelos open-source vs proprietários e como escolher a melhor opção para cada caso!",
  "resources": [
    {
      "type": "link",
      "title": "RAG Paper - Retrieval-Augmented Generation",
      "url": "https://arxiv.org/abs/2005.11401",
      "description": "Paper original sobre RAG"
    },
    {
      "type": "code",
      "title": "pgvector - PostgreSQL Vector Extension",
      "url": "https://github.com/pgvector/pgvector",
      "description": "Extensão para busca vetorial no PostgreSQL"
    },
    {
      "type": "link",
      "title": "Pinecone - Vector Database",
      "url": "https://www.pinecone.io",
      "description": "Vector database gerenciado na nuvem"
    },
    {
      "type": "code",
      "title": "LangChain RAG Tutorial",
      "url": "https://python.langchain.com/docs/use_cases/question_answering/",
      "description": "Tutorial completo de RAG com LangChain"
    },
    {
      "type": "video",
      "title": "RAG Explained - YouTube",
      "url": "https://www.youtube.com/watch?v=example",
      "description": "Explicação visual de como RAG funciona"
    }
  ]
}

